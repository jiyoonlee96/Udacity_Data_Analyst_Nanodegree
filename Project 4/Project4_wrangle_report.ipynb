{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle and Analyze Data: Wrangle Report\n",
    "\n",
    "## 1. Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Wrangle and Analyze Data project, I used Python and its libraries (numpy, pandas, os, requests, tweepy, json) to gather data from a variety of sources. I downloaded the first dataset tweet archive of Twitter user @dog_rates (WeRateDogs) from the Udacity website. @dog_rates is a Twitter account that rates and comments on people's dogs. There are 5000+ tweets in the dataset with variables _tweet_id, timestamp, source, url, rating,_ and _dog stage_. \n",
    "\n",
    "The second datset is the tweet image predictions dataset. I extracted the dataset by running every image in the WeRateDogs Twitter archive through a neural network that classifies the breed of the dog. The variables of the dataset are tweet_id, image URL, top three image predictions, and the image number that corresponds to the most confident prediction (1-4). I downloaded the file hosted on the Udacity's server programatically using the request library and the URL. \n",
    "\n",
    "The third datset is the Twitter API (Application Program Interface) dataset. I used the tweepy library to query Twitter's API for additional variables retweet count, favorite count, followers count, and friends count. Because the Twitter API requires authorization to access, I extracted the Twitter API dataset by setting up a Twitter application. I gathered the tweetJSON with the keys and token provided by the application and downloaded and read in as a dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assessed the three datasets visually and programatically for quality and tidiness issues. Quality issues (also known as content issues) are issues with data completeness, validity, accuracy, and consistency. Tidiness issues are issues with the dataset not following the requirements for tidy data. The three requirements are: each variable forms a column, each observation forms a row, and each type of observational unit forms a table. I found 8 quality issues and 2 tidiness issues:\n",
    " \n",
    "\n",
    "#### **Quality** \n",
    "\n",
    "**_twitter archive_ table**\n",
    "- The datatype of column _tweet_id_ is an integer not an object \n",
    "- The datatype of columns _timestamp_ and _retweeted_status_timestamp_ is an object not a datetime\n",
    "- Columns _in_reply_to_status, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp_ have missing data (null values)\n",
    "- Column _name_ contains invalid names (i.e 'a', 'an', 'the', etc)\n",
    "- Columns _name, doggo, floofer, pupper, puppo_ have 'None'(str) values for null values\n",
    "- Columns _rating_numerator_ and _rating_denominator_ contain invalid and inconsistent values\n",
    "<br>\n",
    "\n",
    "**_tweet image predictions_ table**\n",
    "- The datatype of column _tweet_id_ is an object(str) not a integer\n",
    "- The column names of _p1, p1_conf, p1_dog, p2, p2_conf, p2_dog, p3, p3_conf, p3_dog_ are not descriptive\n",
    "<br>\n",
    "\n",
    "\n",
    "####   **Tidiness**\n",
    "\n",
    "- There are four columns _doggo, floofer, pupper, puppo_ that describe dog stage\n",
    "- The three datasets can be joined as a single dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined, cleaned via coding, and tested each quality and tidiness issues. I changed the datatypes of columnsto the appropriate datatype, dropped unnecessary columns, renamed nondescriptive column names, melted redundant variables to a single variable, and joined the resulting dataframes to create a single master dataframe. I will use the resulting wrangled dataset to find meaninful insights about the data and create visualizations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
